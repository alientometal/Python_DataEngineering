{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Lambda Functions\n",
    "\n",
    "### Problem:\n",
    "\n",
    "Imagine that you are working with a dataset containing information about various transactions. The dataset is a list of dictionaries, where each dictionary contains 'amount' and 'currency' as keys. Your task is to convert all transactions to a specific currency (let's say USD).\n",
    "\n",
    "### Sample Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transactions = [{'amount': 200, 'currency': 'EUR'}, {'amount': 3500, 'currency': 'JPY'}, {'amount': 50, 'currency': 'USD'}]\n",
    "conversion_rates = {'EUR': 1.2, 'JPY': 0.0091, 'USD': 1}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Sample Output:\n",
    "\n",
    "```python\n",
    "converted_transactions = [{'amount': 240.0, 'currency': 'USD'}, {'amount': 31.85, 'currency': 'USD'}, {'amount': 50, 'currency': 'USD'}]\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise: Generators\n",
    "\n",
    "### Problem:\n",
    "\n",
    "When working with large datasets, it is often impossible to load the entire dataset into memory. Therefore, we can use generators to load and process the data in chunks.\n",
    "\n",
    "Your task is to create a generator function that reads a large CSV file and yields a specified number of lines each time it's iterated over.\n",
    "\n",
    "This problem requires you to work with a real CSV file. For the sake of demonstration, I will provide a simplified version of the generator function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_large_file(file_path, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Generator function to read a large CSV file in chunks.\n",
    "    \"\"\"\n",
    "    chunk = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "    for data in chunk:\n",
    "        yield data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You can iterate over this generator function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'large_file.csv'  # replace with your actual file path\n",
    "\n",
    "for chunk in read_large_file(file_path):\n",
    "    # Perform your data processing here\n",
    "    print(chunk.head())  # print the first 5 lines of each chunk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This way, you only have a small portion of the data (a chunk) loaded into memory at a time, which allows you to work with datasets that are larger than your machine's memory. It also allows you to start processing the data immediately, rather than having to wait for the entire dataset to load."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Content created by [**Carlos Cruz-Maldonado**](https://www.linkedin.com/in/carloscruzmaldonado/).  \n",
    "> I am available to answer any questions or provide further assistance.   \n",
    "> Feel free to reach out to me at any time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
